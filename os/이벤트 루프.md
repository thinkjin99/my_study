### 출처

* https://mbinjamil.dev/writings/understanding-async-io/ (이벤트 루프란)
* https://en.wikipedia.org/wiki/Event_loop
* https://baeharam.netlify.app/posts/javascript/event-loop (js의 이벤트 루프)
* https://man7.org/linux/man-pages/man7/epoll.7.html (epoll)
___
### 개요
코루틴


___
### 자바스크립트와 파이썬의 이상함

자바스크립트와 파이썬은 대표적인 싱글 쓰레드 언어이다. **싱글 쓰레드라 함은 한번에 처리하는 명령어가 한개라는 뜻으로** 콜 스택에서 한개의 함수를 실행하고 함수 내부에 존재하는 **명령어를 한개 씩 실행하는 방식으로 동작한다.** 따라서 <span class="red red-bg"><b>어떠한 명령어가 아직 실행중이라면 다른 명령어는 실행될 수 없다.</b></span>

```python
import asyncio
import random

async def do_something(i: int):
    sec = random.randint(1, 10)
    print(f"{i} Wait for {sec}...")  # random wait...
    await asyncio.sleep(sec)  # 이 부분에서 멈춰야 한다!
    print(f"{i} waiting done")


async def main():
    tasks = [asyncio.create_task(do_something(i)) for i in range(3)]
    await asyncio.gather(*tasks) #wait for end...


if __name__ == "__main__":
    asyncio.run(main())

```

하지만 위의 코드를 실행하면 **대기를 진행하는 코드인 sleep이 실행 중임에도 다음 코드 들이 이어서 실행된다.** 심지어 sleep이 완료되면 다시 이전의 실행 위치로 돌아와 함수를 마저 실행하고 종료되는 형태로 동작한다. <b><u>싱글 쓰레드에서 이러한 동작은 귀신 쓰레드가 sleep의 완료를 대기하고 메인 쓰레드는 이후 코드를 실행하는 것 처럼 보여 위화감을 불러 일으킨다.</u></b> 또한 대기가 발생하던 지점으로 어떻게 돌아오는지도 고민을 해봐야 한다. 우리가 위화감을 느낀 부분을 정리하면 아래와 같다.

* **대기는 누가 어떻게 하는 거지?**
* **어떻게 대기 완료 후 실행 흐름이 돌아오는 거지?**
___
### 이벤트 루프

첫번째 질문인 대기는 누가 어떻게 하는 거지에 대해 생각해보자. 알림 스피커가 고장난 전자렌지를 사용한다 생각해보자, 전자렌지가 전부 돌았는지를 확인 하려면 렌지 앞에서 불이 꺼질때까지 지켜보는 수 밖에 없다. 

자 그러면 정리됐다. 누군가 지켜봐야 하니까 아마 대기를 위한 별도의 쓰레드가 존재하고 이를 활용해 처리를 하는 것이 분명한 것 같다. 따라서 **JS와 파이썬은 선량한 개발자를  우롱하는 싱글 쓰레드인 척하는 언어이고 실상은 멀티 쓰레딩이 가능하다는 진실을 우리는 밝혀냈다.** 귀도 반 로섬을 당장 고소해도 승소할 자신감이 생기는 시점이다.

안타깝게도 이걸 통해 소송을 걸 경우 우리가 패소할 확률이 100%이다. 왜냐하면 <span class="red red-bg"><b>실제로 싱글 쓰레드 언어인 것이 맞고 이벤트 루프를 통해 마치 멀티 쓰레드인 것 처럼 동작 시키기 때문이다.</b></span>

이제 이벤트 루프가 무엇인지 알아보자. ==**이벤트 루프는 하나의 구조 혹은 디자인 패턴으로 프로그램의 메시지나 이벤트를 대기하고[[시스템 콜#Context Switching|디스패칭]]하는 작업을 진행한다.  (이벤트를 대기하다가 처리부로 스위칭)**==
이벤트 루프는 이벤트를 대기하다 이벤트가 발생할 경우 해당 이벤트의 핸들러를 호출해 이벤트가 적절히 처리되게 해준다. 이벤트 루프는 파일(fd) 인터페이스 기반으로 작성된 경우가 많으며 이때 사용하는 [[IO Multiplexing|epoll, select]]등의 시스템 콜또한 이벤트 루프를 구성하는데 사용된다.

> [!info]
> **이벤트 루프란 이벤트를 대기하고 이것이 적절히 처리 가능하게 사용하는 구조이다. 루프라는 이름이 붙은 이유는 이벤트가 발생하는지 반복해서 확인하고 적절히 처리해줘야 하기 떄문이다.**

___
### 왜 이벤트 루프?

우리는 파이썬, JS 같은 언어를 예시로 들며 싱글 쓰레드가 여러 개의 IO 작업을 수행하는 것에 대해 의구심을 품었다. 처음에 우리는 이것이 멀티 쓰레딩일 것이라 확신했지만, 이벤트 루프를 통해서 이를 처리할 수 있다 학습했다. 그렇다면 이벤트루프는 어떻게 싱글 쓰레드에서 여러 개의 이벤트에 대응할 수 있게 처리할까?

**싱글 쓰레드에서 여러개의 IO를 처리하기 위한 방법은 앞서 언급한 select, epoll 등을 활용하는 것이다**. 해당 방법들은 약간의 차이가 존재하지만, **==공통적으로 관심있는 이벤트가 발생하는 fd를 등록하고 해당 fd를 지속적으로 모니터링해 이벤트 발생여부를 확인하는 방식으로 동작==한다.**  

만약 10개의 소켓의 데이터 전송을 확인하고 싶으면  <b><u>각 소켓에서 이벤트가 발생 할 때까지 블락하는 것이 아니라, 여러 소켓을 잠깐씩 전부 순회하며 이벤트 발생 여부를 확인</u></b>하는 것이다. 이렇게하면 싱글 쓰레드에서도 여러개의 이벤트를 확인하고 관리하는 것이 가능해진다.

**이벤트 루프는 이러한 방식으로 관심있는 이벤트를 구독한 후 만약 이벤트가 발생할 경우 콜백을 호출해 이벤트의 적절한 처리를 다른 함수에 맡긴다.** 따라서 이벤트 루프를 활용하면 적절한 Mutilplexing을 통해 싱글 쓰레드에서 병행적으로 이벤트를 감지하고 핸들링하는 것이 가능해진다.

```sudo
function main
    initialize()
    while message != quit //루프 시작
        message := get_next_message() //이벤트를 감지한다. epoll,select 사용...
        process_message(message) //이벤트 처리 (콜백 등)
    end while
end function
```
____
### 실전 압축 이벤트 루프

이벤트 루프를 활용해 이벤트를 감지하고 이를 적절히 처리하는 코드를 작성해보자. 아래 코드는 이전의 [[IO Multiplexing#epoll 써먹기|epoll]]에서 사용한 코드로 이벤트 루프를 활용해 싱글 쓰레드에서 여러 소켓과 통신을 가능케 하는 구조이다.

```c
while (1) { // 이벤트 루프의 시작
	int numEvents = epoll_wait(epollfd, events, MAX_EVENTS, -1); //이벤트가 발생할 때 까지 블락
	for (int i = 0; i < numEvents; ++i) {
		if (events[i].data.fd == serverSockfd) { //발생한 이벤트가 리스닝 소켓이면
			// Accept a new connection
			clientSockfd = accept(serverSockfd, (struct sockaddr *)&clientAddr, &clientAddrLen); //연결 수립
			printf("Accepted a new connection from %s:%d\n",
				   inet_ntoa(clientAddr.sin_addr), ntohs(clientAddr.sin_port));

			// Initialize and add the new client to epoll
			initClient(&clients[clientSockfd], clientSockfd); //클라이언트 정보 설정
			event.events = EPOLLIN; //읽기 이벤트 
			event.data.fd = clientSockfd; //수립한 클라이언트 커넥션
			epoll_ctl(epollfd, EPOLL_CTL_ADD, clientSockfd, &event); //이벤트 구독
		} else {
			// Receive data from an existing client
			receiveData(&clients[events[i].data.fd]); //클라이언트에서 이벤트 발생시 
		}
	}
}
```

반복적으로 이벤트의 발생 여부를 체크하고 이벤트가 발생한 fd에 따라 적절한 처리를 해주는 것을 확인할 수 있다. ==**복잡한 것이 이벤트 루프가 아니다. 이벤트의 발생 여부를 주기적으로 확인하고 이에 대한 적절한 처리를 해주는 구조를 이벤트 루프라 부른다.**==
___
### JS의 이벤트 루프

이벤트 루프에 대한 설명글을 찾아보면 대부분이 전부 JS를 예시로 들기 때문에 이벤트 루프를 좁게 정의하게 되는 경향이 존재한다. 하지만 착각하지 말자 JS의 이벤트 루프는 이벤트 루프중 하나일 뿐이다. 

![[Pasted image 20240110152152.png]]

위는 자바스크립트에서 사용하는 이벤트 루프의 예시이다. 우측은 **JS가 아닌 WebAPI에서 처리해주는 요소들을 말하며 비동기 처리되는 함수들이 전부 여기에 위치**한다. **setTimeout같은 함수는 JS엔진이 아닌 브라우저 단에서 처리를 해주며 이는 엔진과 별개로 동작하기 때문에 엔진의 쓰레드를 사용하지 않는다.**

setTimeout이 완료되면 이는 콜백 큐로 이동하게 된다. 콜백 큐는 비동기 처리 작업이 완료된 함수들이 위치한 곳이고 여기에 있는 콜백들은 콜 스택이 비었을 경우 콜백 큐의 함수를 당한다. 이벤트 루프가 해당 작업을 담당하며 루프는 이를 처리하기 위해 콜백 큐와 스택을 항상 감시한다.

**JS의 이벤트 루프는 이벤트의 발생까지 감지 하지는 않고 발생한 이벤트 들의 적절한 처리를 도와주는 이벤트 디스패칭 기능이 특화돼 있다고 보여진다.** 
___
### libuv

libuv는 Node.js를 위해 작성된 이벤트 기반의 비동기 처리 라이브러리이다. 이는 **크로스 플랫폼을 지원하며 각 OS 환경마다 다르게 존재하는 비동기 라이브러리를 전부 한번에 모은 후 최적화해 어느 곳에서나 쉽게 이벤트 기반의 비동기 처리를 가능하게 한다.**

![[Pasted image 20240115160611.png]]

libuv는 파일을 제외한 네트워크 연결에 대해서는 IO 멀티 플렉싱을 지원한다. 파일이나 DNS의 경우는 별도의 쓰레드 풀을 운영해 처리한다.

libuv는 이벤트 루프 구조를 활용해 동작한다. 이때 사용하는 폴링 알고리즘은 각 OS 별로 가장 적합한 폴링 알고리즘을 사용한다. 이외 싱글 쓰레드의 논 블락킹 소켓을 통해 동작하는 매커니즘 자체는 일반적인 비동기-논블락킹 패턴과 동일하다.

![[Pasted image 20240115161637.png]]

libuv의 동작 순서는 단순하다. 정말 간략하게 요약하면 아래와 같다.

1. 시간을 업데이트 한다.
2. 루프가 살아있을 경우 이전에 실행하지 못한 콜백을 실행한다.
3. 이벤트가 발생해 준비된 콜백들을 실행한다.
4. 이벤트를 대기한다.
5. 구독 해지시 처리해야할 콜백을 처리한다.
6. 시간을 업데이트 한다.

#### Handle and Request
**libuv는 크로스-플랫폼을 지원하는 만큼 추상화가 필수적인데, 핸들과 리퀘스트를 사용해 추상화를 진행한다.** 핸들은 루프에 오랫동안 머무는 fd와 같은 등록하는 요소를 의미하고 리퀘스트는 핸들에서 발생한 한번의 IO 이벤트를 말한다. 리퀘스트는 콜백을 통해 사용되며 루프 내부에서 짧게 머문다.

**즉, 핸들은 주기적으로 이벤트를 확인하는 요소이고 리퀘스트는 해당 핸들에서 발생한 이벤트 데이터 정도라고 정의할 수 있다.**
<<<<<<< develop
**즉, 핸들은 주기적으로 이벤트를 확인하는 요소이고 리퀘스트는 해당 핸들에서 발생한 이벤트 데이터 정도라고 정의할 수 있다.**
=======
**즉, 핸들은 주기적으로 이벤트를 확인하는 요소이고 리퀘스트는 해당 핸들에서 발생한 이벤트라고 정의할 수 있다.**

**핸들의 대표적인 예시로는 Stream Handle가 존재하며 tcp, pipe, tty 핸들이 이를 상속받아 구현된다**. [실제 API](https://docs.libuv.org/en/stable/stream.html)를 살펴보면 더 직관적으로 와 닿는데 Stream 핸들에는 다음과 같은 요소들이 존재한다.

* **uv_listen**
	연결을 듣는 작업을 루프에 등록하는 함수로 서버 소켓과 연결 수립시 처리할 콜백을 인자로 받는다. 이 함수를 통해 루프에서 계속해서 listen 이벤트를 확인하며 이벤트 발생시 콜백을 실행해준다.

* **uv_accept**
	연결을 수립하는 작업을 수행한다. **listen 함수의 콜백 내부에서 사용해야만 하는 함수** 이다. 해당 함수는 한 커넥션 당 한번만 호출할 수 있다.

* **uv_read_start**
	읽기 이벤트를 감지할 핸들과 이벤트 발생시 호출할 콜백을 인자로 받는다. 읽기 이벤트를 루프에 등록해 읽을 것이 생기면 콜백을 호출한다.

Stream 핸들은 커넥션을 사용하는 여러 프로토콜에서 공통적으로 사용하는 API들을 구현해놨다. 각 프로토콜 별 요소들은 Stream 핸들을 상속 받은 각 핸들 내부에 추가적으로 구현돼 있다.

<b><u>리퀘스트는 위의 API를 사용하며 등록한 콜백에 전달되는 매개변수로 해당 핸들에서 발생한 IO 이벤트 정보를 갖고 있다.</u></b> 핸들을 통해 데이터를 작성하거나 내보내고 싶다면 리퀘스트를 활용 해야만한다. 리퀘스트는 비동기 IO를 처리하기 위한 자료구조이다. 
___
### libuv 써먹어보기

이벤트 루프를 libuv를 사용해 작성해 사용해보자. 직접 만든 이벤트 루프와 비교하면서 한번 살펴보자. 아래는 간단하게 작성한 TCP Echo Server이다.

```c
#include <stdio.h>
#include <unistd.h>
#include <string.h>
#include <stdlib.h>
#include <uv.h>

uv_loop_t *loop;
struct sockaddr_in addr;

void alloc_buffer(uv_handle_t *handle, size_t suggested_size, uv_buf_t *buf)
{
	//버퍼를 할당한다.
    buf->base = (char *)malloc(suggested_size);
    buf->len = suggested_size;
}

void echo_write(uv_write_t *req, int status)
{
	//write 작성 이후에 콜백으로 오류 검출 등의 작업을 진행한다.
    if (status)
    {
        fprintf(stderr, "Write error %s\n", uv_strerror(status));
    }
    free(req);
}

void echo_read(uv_stream_t *client, ssize_t nread, const uv_buf_t *buf)
{
	// 읽기 작업 이후 실행되는 콜백
    if (nread < 0)
    {
        if (nread != UV_EOF)
        {
	        //연결 종료시
            fprintf(stderr, "Read error %s\n", uv_err_name(nread));
            uv_close((uv_handle_t *)client, NULL); // 핸들을 루프에서 제거한다.
        }
    }
    else if (nread > 0)
    {
        uv_write_t *req = (uv_write_t *)malloc(sizeof(uv_write_t)); // 쓰기용 리퀘스트 객체 생성
        uv_buf_t wrbuf = uv_buf_init(buf->base, nread);             // 쓰기용 버퍼 생성
        printf("buffer: %s\n", buf->base);
        uv_write(req, client, &wrbuf, 1, echo_write); // 버퍼에 내용 작성
    }

    if (buf->base)
    {
        free(buf->base);
    }
}

void on_new_connection(uv_stream_t *server, int status)
{
    if (status < 0)
    {
        fprintf(stderr, "New connection error %s\n", uv_strerror(status));
        return;
    }

    uv_tcp_t *client = (uv_tcp_t *)malloc(sizeof(uv_tcp_t));
    uv_tcp_init(loop, client);                         // 클라이언트 용 tcp 소켓 초기화
    if (uv_accept(server, (uv_stream_t *)client) == 0) // 서버 소켓의 커넥션 클라이언트에 할당
    {
        uv_read_start((uv_stream_t *)client, alloc_buffer, echo_read); // 연결된 커넥션에서 데이터를 읽는 이벤트를 등록한다. 이벤트가 발생할 때 마다 콜백을 실행한다.
    }
    else
    {
        uv_close((uv_handle_t *)client, NULL); //연결 실패시
    }
}

int main()
{
    loop = uv_default_loop();

    uv_tcp_t server;
    uv_tcp_init(loop, &server); // tcp 소켓 초기화

    uv_ip4_addr("127.0.0.1", 7000, &addr);

    uv_tcp_bind(&server, (const struct sockaddr *)&addr, 0);           // 바인딩
    int r = uv_listen((uv_stream_t *)&server, 128, on_new_connection); // 리스닝 작업 등록, 리스닝 이벤트 발생시 on_new_connection 콜백 실행
    if (r)
    {
        fprintf(stderr, "Listen error %s\n", uv_strerror(r));
        return 1;
    }
    return uv_run(loop, UV_RUN_DEFAULT);
}
```

서버 코드를 보면 기존과 가장 크게 느껴지는 점은 반복문이 제거됐다는 점이다. 현재 **코드는 반복문 없이 루프에 이벤트를 등록하고 콜백을 활용해 처리하는 구조를 띄고 있다.** 다만 모든 함수들이 콜백의 방식으로 처리되기 때문에 코드의 흐름이 한눈에 들어오진 않아 **디버깅에 어려움이 존재**한다.

```c
#include <stdio.h>
#include <unistd.h>
#include <string.h>
#include <stdlib.h>
#include <uv.h>

void doEcho(uv_stream_t *tcp);

void alloc_buffer(uv_handle_t *handle, size_t suggested_size, uv_buf_t *buf)
{
    buf->base = malloc(suggested_size);
    buf->len = suggested_size;
}

void echo_read(uv_stream_t *server, ssize_t nread, const uv_buf_t *buf)
{
    if (nread == -1)
    {
        fprintf(stderr, "error echo_read");
        return;
    }

    printf("result: %s\n", buf->base); //에코 메시지 출력
    doEcho(server); //다시 에코 실행
}

void on_write_end(uv_write_t *req, int status)
{
    if (status == -1)
    {
        fprintf(stderr, "error on_write_end");
        return;
    }
    else
        uv_read_start(req->handle, alloc_buffer, echo_read); // 메시지 읽는 이벤트 등록
}

void doEcho(uv_stream_t *tcp)
{
    char buffer[100];
    char message[20];
    uv_buf_t buf = uv_buf_init(buffer, sizeof(buffer));
    printf("input: "); //메시지 작성

    scanf("%s", message);
    buf.len = strlen(message);
    buf.base = message;

    uv_write_t write_req;
    int buf_count = 1;
    uv_write(&write_req, tcp, &buf, buf_count, on_write_end); //메시지 전송 이벤트 등록
}

void on_connect(uv_connect_t *req, int status)
{
	//연결 수립시 진행하는 콜백
    if (status == -1)
    {
        fprintf(stderr, "error on_write_end");
        return;
    }
    else
    {
        printf("Connection Success\n");
        doEcho(req->handle); // 에코 함수 호출, 이벤트가 발생한 핸들을 넘겨준다.
    }
}

int main()
{
    uv_loop_t *loop = uv_default_loop();
    uv_tcp_t *socket = (uv_tcp_t *)malloc(sizeof(uv_tcp_t));
    uv_tcp_init(loop, socket);

    uv_connect_t *connect = (uv_connect_t *)malloc(sizeof(uv_connect_t));
    struct sockaddr_in dest;
    uv_ip4_addr("127.0.0.1", 7000, &dest);

    uv_tcp_connect(connect, socket, (const struct sockaddr *)&dest, on_connect);//서버와 연결 이벤트 등록

    uv_run(loop, UV_RUN_DEFAULT); // 루프 실행
}
```

클라이언트 코드는 반복문 없이 함수끼리 순환하는 구조로 작성해 메시지를 반복해 보내고 있다. <b><u>반복문을 활용하면 곤란한 상황에 빠지기 때문인데 무한 루프 while을 doEcho 함수에서 활용할 경우 루프가 쓰레드를 할당 받지 못하면서 영영 while문에서만 코드가 실행되는 상황이 발생하기 때문이다.</u></b>

<span class="red red-bg"><b>이에 따라 루프의 흐름을 막지 않는 형태로 콜백을 통해서 순환고리를 만드는 방식으로 코드를 작성할 수 밖에 없다. </b></span> on_connect -> doEcho -> on_write_end -> echo_read -> doEcho

문제는 이러한 방식으로 코드를 작성하면 코드의 흐름을 알아채기도 어렵고 디버깅도 어려워진다. 또한 ==**콜백에서 콜백을 호출하고 또 콜백을 호출하는 콜백 지옥에 빠질 가능성이 존재하기 때문에 이를 개선할 필요가 존재한다.**==
___
### 콜백 지옥 벗어나기

위와 같이 콜백에 콜백을 물리는 구조에서 벗어날 수 없는 이유는 조건을 보장할 수 없기 때문이다. 어떠한 이벤트가 발생했을 때 발생하는 것이 콜백인데 콜백의 구조가 아닌 방식으로 함수를 호출하기 위해선 인위적으로 이벤트가 발생했다는 것을 감지해줘야 한다.

```python
while True:
	if is_event():
		do_something()
	
```

이러한 방식을 활용하면 콜백을 활용하지 않고 함수를 호출할 수 있지만, 비지웨이팅이라는 문제점이 발생한다. <span class="red red-bg"><b>이에 따라 비지웨이팅 하지 않고 함수를 블락 했다가 원하는 이벤트가 발생했을 때 다시 돌아오는 구조에 대한 요구가 커져갔는데 이를 만족하기 위해 등장한 것이 async/await이다.</b></span>
>>>>>>> main